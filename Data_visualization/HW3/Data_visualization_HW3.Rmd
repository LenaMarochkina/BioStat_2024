---
title: "Data_visualization_HW_2"
author: "Elena Marochkina"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_depth: 1
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(readr)
library(tidyverse)
library(ggplot2)
library(RColorBrewer)
library(ggpubr)
library(rstatix)
library(ggcorrplot)
library(GGally)
library(pheatmap)
library(factoextra)
library(caret)
library(plotly)
library(umap)
library(gridExtra)


```

# Read and glimpse data

``` {r read and glimpse}
df_init <- readRDS("very_low_birthweight.RDS") %>%
  mutate(ID = as.factor(row_number()),
        across(where(is.integer), as.numeric),
        across(where(is.character), as.factor),
        # Transform some numeric vars to factor (binary variables)
        # Rank variables stay in numeric format: apg1
        across(c(ID, twn, vent, pneumo, pda, cld, dead, magsulf, meth, toc, ), ~ as.factor(.x)))


skimr::skim(df_init)
```

# Task 1

Download the very_low_birthweight.RDS dataset (in your homework folder).
This is data on 671 very low birth weight (<1600 grams) infants collected at Duke University Medical Center by Dr. Michael O'Shea from 1981 to 1987. The outcome variables are the 'dead' column and the time from birth to death or discharge (derived from 'birth' and 'exit'. 7 patients were discharged before birth).
Make a copy of the dataset, remove columns with more than 100 missing values, and then remove all rows with missing values.

``` {r delete missed data}
df <- df_init %>%
  select(where(~ sum(is.na(.)) <= 100)) %>%
  drop_na()

```

# Task 2

Plot density function plots for numeric variables. Remove outliers if any. Transform categorical variables into factors. For any two numeric variables, color the plot by the variable ‘inout’.

``` {r density functions}
# Define remove_outliers function
remove_outliers <- function(x) {
  mean_x <- mean(x)
  sd_x <- sd(x)
  x[x < (mean_x - 3 * sd_x) | x > (mean_x + 3 * sd_x)] <- NA
  return(x)
}

# Apply function
numeric_vars <- sapply(df, is.numeric)

df <- df %>%
  mutate(across(where(is.numeric), remove_outliers)) %>% drop_na()
  
# Plot density functions for numeric variables
numeric_cols <- names(df)[numeric_vars]

for (col in numeric_cols) {
  p <- ggplot(df, aes_string(x = col)) +
    geom_density(fill = "violet", alpha = 0.5) +
    labs(title = paste("Density plot of", col), x = col, y = "Density") +
    theme_minimal()
  print(p)
}

ggplot(df, aes_string(x = "birth", fill = "inout")) +
    geom_density(alpha = 0.4) +
    labs(title = "Density plot of birth", x = "birth", y = "Density") +
    scale_fill_manual(values = brewer.pal(n = 4, name = "Set3")) +
    theme_minimal()

ggplot(df, aes_string(x = "lowph", fill = "inout")) +
    geom_density(alpha = 0.4) +
    labs(title ="Density plot of lowPH", x = "lowPH", y = "Density") +
    scale_fill_manual(values = brewer.pal(n = 4, name = "Set3")) +
    theme_minimal()

```

Somenow hospstay variable has negative number of day. I decided to filter that.

``` {r filter negative hos}
df <- df %>% filter(hospstay >= 0)

```

# Task 3

Conduct a test comparing the values of the 'lowph' column between groups in the inout variable. Determine the type of statistical test yourself. Visualize the result using the 'rstatix' library. How would you interpret the result if you knew that a lower lowph value was associated with lower survival?

``` {r rstatix}
# Perform the t-test
stat.test <- t_test(lowph ~ inout, data = df, var.equal = FALSE, alternative = "two.sided") %>%
  mutate(y.position = 8,
         p = formatC(p, format = "f", digits = 7)) 

# Create box plot
p <- ggboxplot(
  df, x = "inout", y = "lowph")

# Add p-value annotation
p + stat_pvalue_manual(stat.test, label = "T-test, p = {p}", y.position = 8) +
  labs(title = "Comparison of LowPH Between Groups",
       x = "In/Out Group",
       y = "LowPH") +
  theme_minimal()
```

A p-value of < 0.00001 indicates that we can reject the H0 that the mean lowpH between groups "born at Duke" and "transported" are the same. If a lower lowph is associated with worse survival the group with significantly lower lowph values "transported" group likely has a worse survival outcome.

# Task 4 

Make a new dataframe in which you leave only continuous or rank data, except for 'birth', 'year' and 'exit'. Perform a correlation analysis of this data. Plot any two types of graphs to visualize the correlations.

``` {r numeric data correlation}
df_new <- df %>%
  select(where(is.numeric)) %>%
  select(-birth, -year, -exit)
  
correlation_matrix <- cor(df_new, method = "spearman")

# heatmap of the correlation matrix
ggcorrplot(correlation_matrix, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           lab_size = 2, 
           colors = c("darkgreen", "white", "gold"), 
           title = "Correlation Heatmap",
           legend.title = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Define the custom lower panel function
suppressWarnings({
  lower <- function(data, mapping, method = "lm", ...) {
    ggplot(data = data, mapping = mapping) +
      geom_smooth(method = method, color = "darkgreen", se = FALSE, ...)
  }

  # Generate the pair plot
  p <- ggpairs(
    df_new, 
    progress = FALSE,
    lower = list(continuous = wrap(lower, method = "lm")),
    upper = list(continuous = wrap("cor", size = 2))
  ) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
})

print(p)
```

# Task 5 

Build a hierarchical clustering on this dataframe.

``` {r hierarchical clustering}
# Scale the data
df_scaled <- scale(df_new)

# Compute the distance matrix
df_cluster <- dist(df_scaled, method = "euclidean")

# Perform hierarchical clustering 
res.hc <- hclust(df_cluster, method = "ward.D2")

# Plot the dendrogram
plot(res.hc, cex = 0.5, hang = -1, main = "Cluster Dendrogram")
rect.hclust(res.hc, k = 4, border = 2:5)
```

# Task 6

Make a simultaneous plot of heatmap and hierarchical clustering. Interpret the result.

``` {r heatmap + hierarchical clustering}
df_scaled <- scale(df_new)

# Create a heatmap with hierarchical clustering
pheatmap(
  df_scaled, 
  clustering_distance_rows = "euclidean",
  clustering_distance_cols = "euclidean",
  clustering_method = "ward.D2",
  cutree_rows = 4,    
  cutree_cols = 4,    
  color = colorRampPalette(c("darkgreen", "white", "gold"))(50), 
  main = "Heatmap with Hierarchical Clustering",
  fontsize = 10
)
```

Variables grouped together (e.g., gest, bwt or lowph, pltct) might be strongly correlated or share similar distributions across observations.

# Task 7 and 8

Perform PCA analysis on this data. Interpret the result. Should this data be scaled before performing PCA?
Create a biplot for PCA. Color it by the value of the 'dead' column.

``` {r PSA}
df_scaled <- scale(df_new)

# Perform PCA
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)

# PCA Summary
summary(pca_result)

# Visualize variance
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

pca_data <- as.data.frame(pca_result$x)
pca_data$dead <- df$dead

# PCA Biplot with coloring by dead column
fviz_pca_biplot(
  pca_result,
  geom.ind = "point", 
  col.ind = pca_data$dead,  
  palette = c("darkgreen", "gold"),  
  legend.title = "Dead", 
  col.var = "black",         
  repel = TRUE       
) +
  labs(title = "PCA Biplot Colored by 'Dead'", x = "PC1", y = "PC2")
```

PC1 explains 43.2% of the variance, the largest contribution among all components.
PC2 explains 18.1%.
Together, the first 3 components explain about 73.7% of the variance.

- bwt and gest point toward the negative side of PC1, suggesting that higher values of these variables contribute negatively to PC1.
- hospstay points positively along PC1, indicating that longer hospital stays contribute positively to PC1.
- pltct, lowph and apg1 contribute to variance in PC2.
- longer arrows like gest and bwt indicate stronger influence.

# Task 9 
Change the last graph to 'plotly'. When you hover over a point, you want the patient id to be displayed.

```{r task 9}

# PCA data with patient_id
pca_data <- as.data.frame(pca_result$x)
pca_data$dead <- df$dead
pca_data$patient_id <- df$ID

# PCA biplot with plotly
p <- plot_ly(
  data = pca_data,
  x = ~PC1,
  y = ~PC2,
  type = 'scatter',
  mode = 'markers',
  color = ~dead,
  colors = c("darkgreen", "gold"),
  text = ~paste("Patient ID:", patient_id),  # Tooltip text
  hoverinfo = 'text'
) %>%
  layout(
    title = "Interactive PCA Biplot Colored by 'Dead'",
    xaxis = list(title = "PC1"),
    yaxis = list(title = "PC2"),
    legend = list(title = list(text = "Dead"))
  )

p

```

# Task 10

Provide a meaningful interpretation of the PCA analysis. Why is it incorrect to use the 'dead' column to draw conclusions about association with survival?

- hospstay: Positively correlated with PC1, meaning longer hospital stays contribute to higher PC1 values.
- bwt and gest : Negatively correlated, indicating lower values of these variables decrease PC1 values.
- pltct and lowph contribute most to PC2, showing variance unrelated to PC1.

PCA does not consider the target variable (dead) during its computation. It identifies variance in the predictor variables, independent of their relationship to survival. Coloring by dead can visually imply patterns that might not exist statistically. 

# Task 11

Convert your data to two-column dimensions via UMAP. Compare the point mapping results between PCA and UMAP algorithms.

```{r UMAP}
# Perform UMAP 
umap_config <- umap.defaults
umap_config$n_neighbors <- 25  # the number of neighbors
umap_config$min_dist <- 0.005   # the minimum distance
umap_result <- umap(df_scaled, config = umap_config)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

# Add the dead column
umap_data$dead <- df$dead

umap_plot <- ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = as.factor(dead))) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("darkgreen", "gold"), name = "Dead") +
  labs(title = "UMAP: Point Mapping", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

umap_plot

```

# Task 12 

```{r UMAP adjusted}

umap_config <- umap.defaults
umap_config$n_neighbors <- 5  # the number of neighbors
umap_config$min_dist <- 0.5   # the minimum distance
umap_result <- umap(df_scaled, config = umap_config)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

# Add the dead column 
umap_data$dead <- df$dead

umap_plot <- ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = as.factor(dead))) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("darkgreen", "gold"), name = "Dead") +
  labs(title = "UMAP: Point Mapping", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

umap_plot

```

Increasing n_neighbors shows bigger patterns in the data and makes clusters more connected, while decreasing it focuses on smaller, more detailed clusters.

Increasing min_dist spreads points out more and makes clusters less tight, while decreasing it pulls points closer together, creating tighter, more compact clusters.

# Task 13

Let's see for ourselves that dimensionality reduction is a group of methods that is notoriously unstable. Permute 50% and 100% of the 'bwt' column. Run PCA and UMAP analysis. Do you see a change in the PCA cumulative percentage of explained variation? In the resulting biplot presentation of the data for PCA? Is the data visualization different?

```{r }
# Function to permute a percentage of a column
permute_column <- function(data, column, percentage) {
  n <- nrow(data)
  indices <- sample(1:n, size = floor(n * percentage), replace = FALSE)
  data[indices, column] <- sample(data[indices, column])
  return(data)
}

# Original Data
df_original <- df_new
df_original_scaled <- scale(df_new)

# Permute 50% and 100% of bwt
df_50_perm <- permute_column(df_original, "bwt", 0.5)
df_100_perm <- permute_column(df_original, "bwt", 1.0)

# Scale data
df_50_perm_scaled <- scale(df_50_perm[, !(names(df_50_perm) %in% c("ID"))])
df_100_perm_scaled <- scale(df_100_perm[, !(names(df_100_perm) %in% c("ID"))])

# Perform PCA
pca_original <- prcomp(df_original_scaled, center = TRUE, scale. = TRUE)
pca_50_perm <- prcomp(df_50_perm_scaled, center = TRUE, scale. = TRUE)
pca_100_perm <- prcomp(df_100_perm_scaled, center = TRUE, scale. = TRUE)

# Perform UMAP
umap_original <- umap(df_original_scaled)
umap_50_perm <- umap(df_50_perm_scaled)
umap_100_perm <- umap(df_100_perm_scaled)

# Visualize PCA Variance
explained_variance <- data.frame(
  Components = seq_along(pca_original$sdev),
  Original = (pca_original$sdev^2) / sum(pca_original$sdev^2),
  Permuted_50 = (pca_50_perm$sdev^2) / sum(pca_50_perm$sdev^2),
  Permuted_100 = (pca_100_perm$sdev^2) / sum(pca_100_perm$sdev^2)
)

ggplot(explained_variance, aes(x = Components)) +
  geom_line(aes(y = Original, color = "Original")) +
  geom_line(aes(y = Permuted_50, color = "50% Permuted")) +
  geom_line(aes(y = Permuted_100, color = "100% Permuted")) +
  labs(title = "PCA Cumulative Percentage of Explained Variance",
       x = "Principal Component", y = "Explained Variance") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "orange", "red"))

# PCA Biplot for original data
pca_original_biplot <- ggplot(as.data.frame(pca_original$x), aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.7, aes(color = df$dead)) +
  labs(title = "PCA Biplot - Original Data") +
  theme_minimal()

# PCA Biplot for 50% permuted data
pca_50_biplot <- ggplot(as.data.frame(pca_50_perm$x), aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.7, aes(color = df$dead)) +
  labs(title = "PCA Biplot - 50% Permuted Data") +
  theme_minimal()

# PCA Biplot for 100% permuted data
pca_100_biplot <- ggplot(as.data.frame(pca_100_perm$x), aes(x = PC1, y = PC2)) +
  geom_point(alpha = 0.7, aes(color = df$dead)) +
  labs(title = "PCA Biplot - 100% Permuted Data") +
  theme_minimal()

# UMAP for original and permuted Data
umap_original_df <- as.data.frame(umap_original$layout)
umap_50_df <- as.data.frame(umap_50_perm$layout)
umap_100_df <- as.data.frame(umap_100_perm$layout)

umap_plot_original <- ggplot(umap_original_df, aes(x = V1, y = V2, color = df$dead)) +
  geom_point(alpha = 0.7) +
  labs(title = "UMAP - Original Data", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

umap_plot_50 <- ggplot(umap_50_df, aes(x = V1, y = V2, color = df$dead)) +
  geom_point(alpha = 0.7) +
  labs(title = "UMAP - 50% Permuted Data", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

umap_plot_100 <- ggplot(umap_100_df, aes(x = V1, y = V2, color = df$dead)) +
  geom_point(alpha = 0.7) +
  labs(title = "UMAP - 100% Permuted Data", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

# Display results
grid.arrange(pca_original_biplot, pca_50_biplot, pca_100_biplot, nrow = 3)
grid.arrange(umap_plot_original, umap_plot_50, umap_plot_100, nrow = 3)


```

As the degree of permutation increases, the structure in the data weakens. The reduced explained variance in PC1 indicates a loss of meaningful patterns, and the variance becomes more evenly distributed across PCs.
Randomized Data:

The flatter curve for the 100% permuted dataset shows that the principal components lose their ability to capture dominant trends in the data because bwt no longer correlates with other variables.

# Task 14 

Let's do a sensitivity analysis. Run the analysis as in steps 4-6 on the original with all rows with empty values removed (i.e. including columns with more than 100 missing values), and then on the original dataframe with empty values imputed by the mean or median. How do the results differ? What are the advantages and disadvantages of each approach?

```{r imputation}
df_median_imputed <- df_init %>%
  mutate_if(is.numeric, list(~ replace(., is.na(.), median(., na.rm = TRUE))))

```

## Imputed Data
### Task 4 

Make a new dataframe in which you leave only continuous or rank data, except for 'birth', 'year' and 'exit'. Perform a correlation analysis of this data. Plot any two types of graphs to visualize the correlations.

``` {r numeric data correlation 2}
df_new_2 <- df_median_imputed %>%
  select(where(is.numeric)) %>%
  select(-birth, -year, -exit)

# Perform correlation analysis
correlation_matrix <- cor(df_new_2, method = "spearman")

# heatmap 
ggcorrplot(correlation_matrix, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           lab_size = 2, 
           colors = c("darkgreen", "white", "gold"), 
           title = "Correlation Heatmap",
           legend.title = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

# Define the custom lower panel function
suppressWarnings({
  # Define the custom lower panel function
  lower <- function(data, mapping, method = "lm", ...) {
    ggplot(data = data, mapping = mapping) +
      geom_smooth(method = method, color = "darkgreen", se = FALSE, ...)
  }

  # Generate the pair plot
  p <- ggpairs(
    df_new_2, 
    progress = FALSE,
    lower = list(continuous = wrap(lower, method = "lm")),
    upper = list(continuous = wrap("cor", size = 2))
  ) +
    theme_bw() +
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
})

print(p)
```

One variable was added to the analysis (lol).


### Task 5 

Build a hierarchical clustering on this dataframe.

``` {r hierarchical clustering 2}
# Scale the data
df_scaled <- scale(df_new_2)

# distance matrix
df_cluster <- dist(df_scaled, method = "euclidean")

# Perform hierarchical clustering
res.hc <- hclust(df_cluster, method = "ward.D2")

plot(res.hc, cex = 0.5, hang = -1, main = "Cluster Dendrogram")
rect.hclust(res.hc, k = 4, border = 2:5)
```

### Task 6

Make a simultaneous plot of heatmap and hierarchical clustering. Interpret the result.

``` {r heatmap + hierarchical clustering 2}
# Standardize the data
df_scaled <- scale(df_new_2)

# Create a heatmap with hierarchical clustering
pheatmap(
  df_scaled, 
  clustering_distance_rows = "euclidean",
  clustering_distance_cols = "euclidean",
  clustering_method = "ward.D2",
  cutree_rows = 4,    
  cutree_cols = 4,    
  color = colorRampPalette(c("darkgreen", "white", "gold"))(50), 
  main = "Heatmap with Hierarchical Clustering",
  fontsize = 10
)
```

When imputing missing values, more columns are retained in the analysis (+ lol).

Imputation using the median has suppressed natural variability in the data, and scaling has amplified this issue, leading to clustering results that are less interpretable and visually uniform.

# Task 15

``` {r PSA 2}

# Scale the data
df_scaled <- scale(df_new_2)

# Perform PCA
pca_result <- prcomp(df_scaled, center = TRUE, scale. = TRUE)

# PCA Summary
summary(pca_result)

# Visualize explained variance
fviz_eig(pca_result, addlabels = TRUE, ylim = c(0, 50))

pca_data <- as.data.frame(pca_result$x)
pca_data$dead <- df_init$dead

# PCA Biplot with coloring by dead column
fviz_pca_biplot(
  pca_result,
  geom.ind = "point", 
  col.ind = pca_data$dead,  
  palette = c("darkgreen", "gold"),  
  legend.title = "Dead", 
  col.var = "black",         
  repel = TRUE       
) +
  labs(title = "PCA Biplot Colored by 'Dead'", x = "PC1", y = "PC2")
```

Previously, 1 PSA component explained 43.2% of variations, now 33.8%.

By filling in missing values, the variance across variables can shift, causing PCA to reorient the components.

Imputation added more data, filling gaps and altering the data’s variance structure. This changed how PCA oriented the components, flipping some vars while maintaining the same relative relationships between variables and observations.


```{r UMAP 2}

# Perform UMAP 
umap_config <- umap.defaults
umap_config$n_neighbors <- 25  # the number of neighbors
umap_config$min_dist <- 0.01   # the minimum distance
umap_result <- umap(df_scaled, config = umap_config)

umap_data <- as.data.frame(umap_result$layout)
colnames(umap_data) <- c("UMAP1", "UMAP2")

# Add the dead column
umap_data$dead <- df_init$dead

umap_plot <- ggplot(umap_data, aes(x = UMAP1, y = UMAP2, color = as.factor(dead))) +
  geom_point(alpha = 0.7) +
  scale_color_manual(values = c("darkgreen", "gold"), name = "Dead") +
  labs(title = "UMAP: Point Mapping", x = "UMAP1", y = "UMAP2") +
  theme_minimal()

umap_plot

```

Clasters became more clearer.

